% Chapter containing details about the hysteresis of the muscle
% Outline:
% - Introduction
% - Evolutionary algorithms outline
%	- Genetic Algorithm
% - Multi-variable optimization
% 	- Firefly Algorithm
% 	- Modified Firefly Algorithm
% 	- PSO
% - Implementation
% - Results comparison

\chapter{Hysteresis Parameter Optimization}
\label{ch:optimization}

In this chapter, the methods to obtain the model's parameters are explained.
As shown in Section~\ref{sec:4.eva}, although parameters cherry-picked by trial
and error proved successful to fit the approximated model onto experimental data,
a higher grade of precision is desirable for applications demanding high accuracy.

Section~\ref{sec:5.evo} introduces the concept of \textit{evolutionary algorithm},
with particular regard towards a simple genetic algorithm approach
with the purpose of optimization.

After that, Section~\ref{sec:5.opt} introduces methods of \textit{metaheuristic
algorithm}, with the goal of transforming the search of parameters
into an optimization problem.

Section~\ref{sec:5.res} contains results and comparison
for all algorithms previously described.


\section{Introduction}
After having described how to get a discrete model of the McKibben artificial muscle,
the Bouc-Wen model of hysteresis has been introduced.
This model proves useful to describe a hysteretic behaviour in a simple manner,
and can provide great advantages in terms of performance when it comes to control.

However, there are parameters that have to be tuned in order for the model
to operate correctly. The tuning may be done by trial and error, or the whole process
may be automated.

The parameter refinement process makes use of an algorithm to
find the parameters of the Bouc-Wen model that give a best approximation
of the real experimental data. Such algorithms include, but are not limited to,
Recursive Least Squares (RLS), evolutionary algorithms
and multi-variable optimization algorithms.

For this work, different approaches based on evolutionary algorithms
and multi-variable optimization methods have been tested.

%\clearpage

\section{Evolutionary Algorithms}
\label{sec:5.evo}

An evolutionary algorithm is a population-based optimization algorithm
that mimics the behaviour of biological evolution, simulating the steps of
reproduction, mutation, recombination and selection.

Each individual is tested in an optimization problem,
and the best characteristic of the population are passed through the generations.
The evaluation of an individual is done by means of a fitness function.
A fitness function is a operator that assigns a fitness value to each individual.
Depending on the nature of the problem, individuals with lower (or higher)
fitness values are judged "better" from the algorithm and have a higher probability
of being chosen in the mating step to generate new, better individuals.
Therefore, their characteristics have a higher chance to be passed onto next generations
in order to iteratively refine the parameters and arrive at a acceptable solution.

After a certain amount of generations, or after one individual reaches
a fixed fit value, the algorithm stops and the best individual is chosen
as the solution to the optimization problem.

\subsection{Genetic Algorithm}

A genetic algorithm (GA)~\cite{fleming2001genetic} is a kind of evolutionary algorithm inspired
by the concept of natural selection that permeates a biological evolutionary process.

Genetic algorithms are a class of stochastic global search algorithm
operating on a set called \textit{population} of current approximations,
called \textit{individuals}.
Individuals are encoded as a set (\textit{chromosome}) of parameters (\textit{genes}).

Once a population has been created, each individual performance is assessed
according to the objective function characterising the problem that has to be solved.

Algorithms belonging to this class are divided into the following steps:
\textit{initialization}, \textit{evaluation}, \textit{crossover} and \textit{mutation}.
The general outline of the procedure is shown in Algorithm~\ref{alg:ga}.
The description and peculiarities of each step are discussed in Section~\ref{sec:5.steps}.
The genetic algorithm also makes use of several parameters, which are summarized
in Table~\ref{tab:ga_params}.

% todo: replicare una tabella simile per gli algoritmi metaeuristici

\begin{table}[]
	\centering
	\begin{tabularx}{\linewidth}{c X}
		\toprule
		\textbf{Parameter} & \textbf{Description} \\ \midrule
		$N$        & The population's size, i.e. the number of individuals (candidate solutions) that evolve to find a good approximation. \\
		$G$         & The maximum generation. After reaching that stage of evolution, the algorithm stops. \\
		$d$           	   & The chromosome's dimension, i.e. the number of variables to be optimized by the algorithm. \\
		$L$           	   & A $d$-dimensional vector containing the lower bounds for each variable. \\
		$U$				   & A $d$-dimensional vector containing the upper bounds for each variable. \\
		$P_{cross}$		   & The crossover probability. It controls the chance of inheriting a chromosome from a specific parent with respect to the other. \\
		$P_{mut}$		   & The mutation probability. \\
		$m_m$			   & The mutation magnitude, i.e. how much a given chromosome changes in the event of a mutation. \\ \bottomrule
	\end{tabularx}
	\caption{Parameters for the genetic algorithm}
	\label{tab:ga_params}
\end{table}

\subsubsection{Algorithm Overview}

\begin{algorithm}
	\caption{Genetic algorithm approach} \label{alg:ga}
	\begin{algorithmic}
		\Procedure{Genetic Algorithm}{}\newline
		\textbf{Input:} $N$, $G$, $d$, $L$, $U$, $p_{cross}$, $p_{mut}$, $m_m$\newline
		\textbf{Output:} \textit{A $d$-dimensional vector, i.e. the candidate solution (individual) with best fit value}
		\State{\textbf{Initialization:} \textit{generate $N$ $d$-dimensional vectors} $n_i, i \in \left[1,N\right]$}
		\State{$current\_gen \gets 1$}
		\While{$current\_gen < G$}
			\State{\textbf{Evaluation:} \textit{compute the fit value $fit(i)$ for each vector}}
			\State{\textit{Compute the normalized fit value $fit_n(i)$ for each vector}}
			\State{$M_p \gets$ \textit{new empty Set}}
			\While{$\lvert \mathcal M_p \rvert<N$}
				\State{\textbf{Crossover:} \textit{extract 2 individuals}}
				\State{\textit{Generate 2 new individuals (offspring)}}
				\State{\textit{Add the offspring to} $M_P$}
			\EndWhile{\textbf{end while}}
			\State{\textbf{Mutation:}}
			\ForEach{$m \in \mathcal M_p$ }
				\ForEach{\textit{gene} $m_i, i=1,\ldots,d$}
					\State{\textit{Generate a random value $r$ such that} $0\le r \le 1$}
					\If{$r\le P_{mut}$}
						\State{\textit{Mutate the i-th gene with percentage} $\pm m_m\%$}
					\EndIf
				\EndFor
			\EndFor
		\EndWhile{\textbf{end while}}
		\State{\textit{Return the individual with best fit value}}
		\EndProcedure
	\end{algorithmic}	
\end{algorithm}

\subsubsection{Algorithm Description}
%TODO inserire la descrizione dell'algoritmo genetico

\label{sec:5.steps}
\subsubsection{Initialization Step}

For a genetic algorithm, the initialization step consists in the creation
of the individuals, and by extension that of the population.

One of the most common initialization methods is random initialization:
each gene of any individual is randomly chosen between a defined interval.
This gives a higher chance of finding good results from the first generations,
and refine the solution starting from them.

Another initialization method may be to create a population of equal
individuals, and mainly rely on mutation for the first generations
in order to find better approximations. This method, while it may better
reflect the natural behaviour of a biological evolutionary process,
takes also a much larger number of generations to return appreciable results.

For this reason, the GA developed for this work makes use of random initialization.
$N$ $d$-dimensional vectors (individuals) $p$ are created,
following the constraints in Equation~\ref{eq:constraints}.

\begin{align}
\begin{cases}
	p_i = \left[p_{i1},\ldots,p_{id}\right] \\
	L_j \leq p_{ij} \leq U_j, \quad i=1,\ldots,N, \quad j=1,\ldots,d
\end{cases}
\label{eq:constraints}
\end{align}


\subsubsection{Evaluation Step}

The evaluation step concerns assessing a fitness value to each individual
in order to rank them according to their performance in solving the optimization problem.
The fitness value is assigned to each individual by means of a \textit{fitness function}.

Experimental input pressure and output displacement,
respectively described in Equations~\ref{eq:exp_pres} and~\ref{eq:exp_disp},
are taken from data acquired by the identification experiments described
in Section~\ref{sec:4.ide}. 

\begin{align}
u &= \left[u_1,u_2,\ldots,u_k\right] \label{eq:exp_pres} \\
l &= \left[l_1,l_2,\ldots,l_k\right] \label{eq:exp_disp}
\end{align}

with $k$ representing the number of points for which the experimental data
has been sampled.

Each individual is used to perform a specific simulation of the model with its genes
set as Bouc-Wen hysteresis parameters, using $u$ as input.
The simulated output displacement $\hat{l}$
is then compared to the experimental displacement $l$.

The fitness function chosen for this step is described in Equation~\ref{eq:fitness}.
This function returns the fitness value for the i-th individual of the population.

\begin{align}
fit(i) = \frac{1}{\sum_{i=1}^k{\left(l_i-\hat{l}_i\right)^2}}
\label{eq:fitness}
\end{align}

Judging from the nature of the fitness function, it is clear that
individuals with a fitness value close to 0 are better than others,
as it means that their simulated output is close to experimental data.

After that, the fit value of each individual is normalized in order to be used
for the crossover step. To do so, each fit value is divided by the sum of all fit values,
as shown in Equation~\ref{eq:fit_norm}.

\begin{align}
fit_n(i) = \frac{fit(i)}{\sum_{i=1}^n fit(i)}
\label{eq:fit_norm}
\end{align}

It is important to notice that given the way that $fit_n$ is constructed,
the relation of Equation~\ref{eq:sum_1} arises.

\begin{align}
\sum_{i=1}^N fit_n(i) = 1
\label{eq:sum_1}
\end{align}

\subsubsection{Crossover Step}

% TODO: Descrivere il crossover, comprendendo le varie probabilità e i 3 tipi diversi di crossover

The crossover step serves the purpose of creating two new individuals (\textit{offspring})
starting from two \textit{parents}. The characteristics of the offspring will thus
largely depend on their parents' ones.

Three different crossover policies have been developed for this purpose:

%TODO: Sarebbe bello inserire delle immagini per spiegare il riposizionamento dei vari geni, almeno per quanto riguarda il random crossover

\begin{itemize}[noitemsep]
	\item \textbf{Random crossover}
	
	The random method assigns the genes from parents to offspring in a random fashion.
	
	Given a number $x$ with $1 \le x \le d$,
	the offspring inherits the first $x$ genes from the first parent and
	the remaining ones from the second parent.
	This process is repeated for the second offspring.
	\item \textbf{Weighted crossover}
	
	The weighted method makes so that the genes of the parent with higher
	fit have a higher probability of being chosen for the offspring.
	
	Given the fit values of the parents $fit(p_1)$ and $fit(p_2)$,
	with $fit(p_1) \ge fit(p_2)$, then the \textit{inheritance probability} $In_p$
	is defined as follows:
	
	\begin{align*}
	In_p = \frac{fit(p_1)}{fit(p_1)+fit(p_2)}
	\end{align*}
	
	For each offspring and gene, the probability of inheriting the given gene
	from the first parent will be $In_p$. Otherwise, the gene will be inherited
	from the second parent.
	\item \textbf{Proportional crossover}
	
	The proportional method creates the offspring's genes from the parents' ones,
	proportionally inheriting them based on their fit values.
	
	Given the fit values of the parents $fit(p_1)$ and $fit(p_2)$,
	with $fit(p_1) \ge fit(p_2)$, then the \textit{proportion variables} $pv_1$
	and $pv_2$ are defined as follows:
		
	\begin{align*}
	pv_1 = \frac{fit(p_1)}{fit(p_1)+fit(p_2)},\quad pv_2 = 1-pv_1
	\end{align*}
	
	Given $Gp_1(x), Gp_2(x), Go(x), 1\le x\le d$
	respectively for the first parent's, the second parent's
	and the offspring's	$x$-th gene, then the following relation arises:
	
	\begin{align*}
		Go(x) = Gp_1(x)\cdot pv_1 + Gp_2(x) \cdot pv_2
	\end{align*}

	This means that the offspring will be more similar
	to the parent with higher fit, while still carrying on some of 
	the less performing parent's characteristics.
\end{itemize}


\subsubsection{Mutation Step}

The mutation step's purpose is to randomly mutate some genes to reflect
the random behaviour of biological evolution through time. 

For each gene $Go(x), 1\le x \le d$ of each offspring generated,
a random value $r$ between $0$ and $1$ is generated. If this value
is less than the \textit{mutation probability} $P_{mut}$, then
the gene is modified by a percentage of its value.

The final value $Go(x)'$ of the x-th gene will be:

\begin{align*}
Go(x)' = Go(x)\cdot \left(1 \pm m_m\right)
\end{align*}

Here the \textit{mutation magnitude} is added or subtracted
with probability $0.5$, by generating a new random value and
verifying whether it surpasses this threshold.

\section{Metaheuristic Algorithms}
\label{sec:5.opt}

A \textit{metaheuristic} is a procedure designed to find a heuristic
that may provide a sufficiently good solution to an optimization problem.

Metaheuristic algorithms do not guarantee that a globally optimal solution
can be found on some class of problems, but they often provide a solution
with a good fit value by exploring the space of possible solutions.

For this work, two metaheuristic algorithms have been developed and tested
with the purpose of finding good parameters for fitting the Bouc-Wen hysteresis
parameters of the identified transfer function over real experimental data.

The first is the \textit{Firefly Algorithm}, described in Subsection~\ref{sec:5.fa}.
This algorithm mimics the movement of a swarm of fireflies inside the solution space,
simulating an \textit{attractiveness component}, governed by their \textit{luminosity},
that pushes the swarm towards local and global optimal solutions.
Two versions of this algorithm have been developed in order to grant a higher
versatility for finding the parameters, depending on the required application.
The modified version is described thoroughly in Section~\ref{sec:5.mfa}.

The second algorithm is the \textit{Particle Swarm Optimization},
described in Section~\ref{sec:5.pso}. It is a computational method that optimizes a
problem by iteratively trying to improve a candidate solution with regard to a given
measure of quality. It solves a problem by having a population of candidate solutions
(\textit{particles}) moving around in the search-space according to simple laws
governed by their position and velocity.
Each particle's movement is influenced by its local best known position,
but is also guided toward the best known positions in the search-space.
This value is updated whenever a better position is found any particle.
This is expected to move the swarm toward the best solutions.
The algorithm has been conceived as a mathematical representation
of the movement of organisms in a bird flock or fish school.

The values chosen for algorithms' parameters are discussed in Section~\ref{sec:5.res},
which is dedicated to show the results and compare these algorithms.


\subsection{Firefly Algorithm}
\label{sec:5.fa}

The Firefly Algorithm (FA) is a metaheuristic optimization algorithm
developed by Xin-She Yang~\cite{yang2010nature} that mimics the behaviour
of fireflies.

The main concept of such algorithm is to create a population of fireflies
representing candidate solutions in a $d$-dimensional space 
for a certain optimization problem, where $d$ is the number of variables to be optimized.

The fireflies are then attracted to each other following a set of rules:

\begin{itemize}[noitemsep]
	\item \textbf{Unisexuality} 
	
	All fireflies are unisex, so that one firefly is attracted
	to other fireflies regardless of their sex.
	\item \textbf{Attractiveness}
	
	The attractiveness of a firefly is proportional to its brightness,
	thus for any two fireflies the less bright will move towards the brighter one.
	Both brightness and attractiveness decrease as the distance between
	two fireflies increases.
	\item \textbf{Brightness}
	
	The brightness of a firefly is determined and affected by the objective function.
\end{itemize}


\subsubsection{Algorithm Overview}

\begin{algorithm}
	\caption{Firefly Algorithm Approach} \label{alg:fa}
	\begin{algorithmic}
		\Procedure{Firefly Algorithm}{}\newline
		\textbf{Input:} $N$, $G$, $d$, $L$, $U$ \newline
		\textbf{Output:} \textit{A $d$-dimensional vector,
			i.e. the candidate solution (firefly) with best fit value}
		\State{\textbf{Initialization:} \textit{generate $N$ $d$-dimensional vectors} $n_i, i \in \left[1,N\right]$}
		\State{\textit{Light intensity} $I_i$ for $n_i$
			\textit{is determined by} $\mathit{f}(n_i)$}
		\State{\textit{Define light absorption coefficient $\sigma$}}
		\State{$current\_gen \gets 1$}
		\While{$current\_gen < G$}
			\For{$i = 1,\ldots,N$}
				\For{$j = 1,\ldots,N$}
					\If{$I_i < I_j$}
						\State{\textit{Move firefly i towards j}}
						\EndIf
					\State{\textit{Vary attractiveness with distance} $r$}
					\State{\textit{Evaluate new solutions and update light intensity}}
				\EndFor
				\State{\textbf{end for} $j$}
			\EndFor
			\State{\textbf{end for} $i$}
			\State{\textit{Rank the fireflies and find the current global best $g^*$}}
		\EndWhile
		\State{\textbf{end while}}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Algorithm Description}

The following sections serves the purpose of describing in depth the steps
and the specifics of the Firefly Algorithm.

Section~\ref{sec:5.light_attract} describes how light intensity influences
the relative attractiveness between two fireflies. 

Section~\ref{sec:5.famovement} describes how fireflies move in the solution
space through the iterations.

Table~\ref{tab:fa_params} summarizes and describes the parameters used by the algorithm.

\begin{table}[]
	\centering
	\begin{tabularx}{\linewidth}{c X}
		\toprule
		\textbf{Parameter} & \textbf{Description} \\ \midrule
		$N$        & The population's size, i.e. the number of fireflies (candidate solutions) that move in the solution space to find a good solution. \\
		$G$         & The maximum generation. After reaching that stage of evolution, the algorithm stops. \\
		$d$           	   & The chromosome's dimension, i.e. the number of variables to be optimized by the algorithm. \\
		$L$           	   & A $d$-dimensional vector containing the lower bounds for each variable. \\
		$U$				   & A $d$-dimensional vector containing the upper bounds for each variable. \\ 
		$\Lambda_0$ & Attractiveness at zero distance, described in Section~\ref{sec:5.azd} \\
		$\sigma$		& Light absorption coefficient that influences the attractiveness distance, described in Section~\ref{sec:5.lac}. \\
		$\rho$ & Randomization parameter influencing the fireflies' motion, described in Section~\ref{sec:5.rp} \\ \bottomrule
	\end{tabularx}
	\caption{Parameters for the genetic algorithm}
	\label{tab:fa_params}
\end{table}

\subsubsection{Light Intensity and Attractiveness}
\label{sec:5.light_attract}

The brightness $I$ of a firefly at a particular location depends
on the fitness value returned by the fitness function at that specific location.

Each firefly follows a path that brings it closer to other
attractive fireflies. Relative attractiveness $\Lambda_{ij}$ however is relative,
and it will vary with the distance $r_{ij}$ between firefly \textit{i} and \textit{j}.

Equation~\ref{eq:attractiveness} shows the relation between distance and attractiveness,
where $\Lambda_0$ is the attractiveness at zero distance and $\sigma$ is
a parameter called \textit{light absorption coefficient}.

\begin{align}
\label{eq:attractiveness}
\Lambda_{ij}=
\begin{cases}
\Lambda_0 e^{-\sigma r_{ij}^2} & \text{if } I_j > I_i \\
0 & \text{otherwise}
\end{cases}
\end{align}

Equation~\ref{eq:attractiveness} suggests that the brighter is a firefly,
the more attractive it is to any other firefly. The light intensity $I_j$
for the \textit{j}-th firefly is determined by the fit function.

\begin{align}
I_j = \mathit{f}_{fit}(n_j) = \mathit{f}_{fit}(n_{j,1},n_{j,2},\ldots,n_{j,d})
\end{align}


The fit function chosen for this work is the same as for the Genetic Algorithm
approach, and is shown in Equation~\ref{eq:fitness}. 
The distance $r_{ij}$ between firefly \textit{i} and \textit{j}
at step \textit{k} is given by the Euclidean distance between the two.

\begin{equation}
\begin{cases}
r_{ij}^k &= \norm{n_j^k-n_i^k} \\ 
&= \sqrt{(n_{j,1}^k-n_{i,1}^k)^2+(n_{j,2}^k-n_{i,2}^k)^2+\ldots+(n_{j,d}^k-n_{i,d}^k)^2}
\end{cases}
\end{equation}

\subsubsection{Firefly Movement}
\label{sec:5.famovement}

The movement of a firefly $n_i$ towards a more attractive $n_j$ 
at step $k \in \left[1,G\right]$ is given by Equation~\ref{eq:fa_movement}.

\begin{align}
\label{eq:fa_movement}
n_i^k = n_i^k + \Lambda_{ij}^k\left(n_j^k-n_i^k\right)+\rho \mathit{v}_i^k
\end{align}

Here $\Lambda_{ij}$ is the attractiveness of firefly \textit{j} to firefly \textit{i},
$\rho$ is a \textit{randomization parameter} and $\mathit{v}_i$ is
a vector of $d$ random numbers drawn from a Gaussian or uniform distribution.

It is important to notice that the movement of a given firefly
towards a less attractive one is given only by a random factor, but it is still present.
This means that the most attractive fireflies still move and can
potentially find better solutions.

After the fireflies' positions are updated, new fitness values
are calculated. New generations are obtained
by using the same set of equations. Over successive generations,
the fireflies converge to local and global optimal values,
completing the optimization process.


\subsection{Modified Firefly Algorithm}
\label{sec:5.mfa}

A modified version of the Firefly Algorithm has been developed and studied
by M.A. Zaman and U. Sikder~\cite{zaman2015bouc}. The main idea behind
the Modified Firefly Algorithm is to change the \textit{process control parameters}
$\Lambda_0$, $\sigma$ and $\rho$ through the iterations. Typical values
of these parameters are $\Lambda_0=1$, $\sigma = 1$ and $\rho \in \left[0.1,0.2\right]$.
By changing these from static to dynamic parameters it is possible to perform
a more precise tuning of the fireflies behaviour, reflecting thus on the performance
of the algorithm on finding their optimal values.

\subsubsection{Attractiveness at Zero Distance}
\label{sec:5.azd}

It has been studied that the attractiveness at zero distance parameter $\Lambda_0$
controls the \textit{exploration distance} of the fireflies: 
higher values of $\Lambda_0$ encourage global exploration while
lower values encourage close refinement (local exploitation). 

Hence it would be better to make $\Lambda_0$ a decreasing value over the iterations, 
such that initially the fireflies explore a wide range of possible solutions
minimizing the risk of getting stuck in a local optimum, while 
near the end of the execution a lower value would allow for fine refinement
around the different optima found.

The formula for the attractiveness parameter for iteration
$k\in \left[1,G\right]$ has been changed as shown in Equation~\ref{eq:attr_changed}.

\begin{align}
\label{eq:attr_changed}
\Lambda_0(k) = a - b\frac{1-k}{1-G}
\end{align}

From Equation~\ref{eq:attr_changed} it is possible to notice
that the value of $\Lambda_0$ goes from $a$ (when $k=1$) to $a-b$ (when $k=G$).

\subsubsection{Light Absorption Coefficient}
\label{sec:5.lac}

The value of $\sigma$ control the effect of distance on the motion of fireflies.
Equation~\ref{eq:attractiveness} suggests that lower values of $\sigma$ will
cause movement towards other attractive fireflies even if they are far away.
This may lead to a premature convergence of a high number of fireflies
towards a local maximum.

However, after a sufficient number of generations, it is desirable
that the fireflies start to converge towards the maximum value found so far.
To do so, a lower value of $\sigma$ is required.

The formula for the \textit{light absorption coefficient} $\sigma$
for iteration $k$ has been changed as shown in Equation~\ref{eq:lac_changed}.

\begin{align}
\label{eq:lac_changed}
\sigma(k) = 2 \cdot \left(7^{\nicefrac{-k}{G}}\right),\quad c \ge 0, d \ge 0
\end{align}

\subsubsection{Randomization Parameter}
\label{sec:5.rp}

The \textit{randomization parameter} $\rho$ controls the random component
of the firefly's motion. A large value of $\rho$ encourages broad random exploration.
Using the same arguments as for the \textit{light absorption coefficient},
it can be inferred that a gradual decrease of the value of $\rho$ over
successive iterations should improve the performance of the algorithm
as the fireflies must explore the solution space widely at the beginning
to find good candidate solutions and then refine them in latter steps.

The formula for the \textit{randomization parameter} $\rho$
for iteration $k$ has been changed as shown in Equation~\ref{eq:random_changed}.

\begin{align}
\label{eq:random_changed}
\rho(k) = 0.1 \cdot e^{\nicefrac{-4.8k}{G}}
\end{align}

The proposed modification of the algorithm using dynamic parameters
are expected to provide a good mix of global exploration and local exploitation,
resulting in high accuracy and fast convergence of the algorithm towards
the global optimum.

\subsection{Particle Swarm Optimization}
\label{sec:5.pso}

Particle Swarm Optimization (PSO) is an evolutionary computation technique
developed by Kennedy and Eberhart in 1995~\cite{kennedy2011particle}~\cite{eberhart1995new}.

PSO is similar to a genetic algorithm, meaning that both algorithms 
are initialized with a population of random solutions. Unlike a GA, however,
candidate solutions, called \textit{particles}, are also assigned a randomized velocity.
After that, they are flown into the problem space.

Each point of the problem space represents a set of parameters for the optimization
problem that has to be solved, in this case the Bouc-Wen model parameter search
Each particle keeps track of its coordinates in the problem space,
and remembers the coordinate of the point which has yielded the best solution,
in other words the point that has the highest fitness value found by them so far.
This value is called \textit{pbest}, which means \textit{personal best}.

Another value is tracked and known by all particles, which is the \textit{global best}
value found by any particle up until that certain iteration of the algorithm.
This value is called \textit{gbest}.

The particle swarm optimization concept consists of, at each time step,
changing the velocity (accelerating) each particle towards its \textit{pbest}
and \textit{gbest} locations. Acceleration is weighted by a random term,
with separate random numbers generated to dictate acceleration towards
\textit{pbest} and \textit{gebest} locations.

% todo: considerare se è il caso di inserire la descrizione di ogni algoritmo dentro la overview piuttosto che all'inizio della macro sezione.

\subsubsection{Algorithm Overview}

\begin{algorithm}[H]
	\caption{Particle swarm optimization approach} \label{alg:pso}
	\begin{algorithmic}
		\Procedure{Particle Swarm Optimization}{}\newline
			\textbf{Input:} $N$, $G$, $d$, $L$, $U$, $V_{max}$\newline
			\textbf{Output:} \textit{A $d$-dimensional vector, i.e. the candidate solution (particle) with best fit value}
			\State{\textbf{Initialization:} \textit{generate $N$ $d$-dimensional vectors} $n_i, i \in \left[1,N\right]$}
			\State{$current\_gen \gets 1$}
			\While{$current\_gen < G$}
				\State{\textbf{Evaluation:} \textit{compute the fit value $fit(i)$ for each vector}}
				\State \begin{varwidth}[t]{.85\textwidth} 
					\textbf{Pbest comparison:} \textit{Compare the particle's fit value with particle's $pbest$.
						If the current value is better than $pbest$, then set $pbest$ value equal to the current value, and $pbest$ location equal to the particle's current location in the $d$-dimensional space}
					\end{varwidth}
				\State \begin{varwidth}[t]{.85\textwidth} 
					\textbf{Gbest comparison:} \textit{Compare the particle's fit value with $gbest$.
						If the current value is better than $gbest$, then set $gbest$ value equal to the current value, and $gbest$ location equal to the particle's current location in the $d$-dimensional space}
					\end{varwidth}
				\State \textbf{Update:} \textit{change the particle's velocity and position}
			\EndWhile{\textbf{end while}}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Algorithm Description}

The following sections serve the purpose of describing in depth the steps
and the specifics of the Particle Swarm Optimization algorithm.

Section~\ref{sec:5.pso.speed} describes how the values for velocity and position
for the particles are updated each iteration.

Section~\ref{sec:5.inertia} describes the purpose of and how to compute
the inertia weight $w$ used when calculating the speed of a particle.

Section~\ref{sec:5.pso.constants} describes the purpose of the acceleration
constants $c_1$ and $c_2$.

\subsubsection{Velocity and Position Update}
\label{sec:5.pso.speed}

Given $x_i$ and $v_i$ which are respectively the \textit{i}-th particle's position
and velocity, then the movement of the \textit{i}-th particle is dictated by
Equations~\ref{eq:pso_velocity} and~\ref{eq:pso_position}.
In Equation~\ref{eq:pso_velocity}, $w$ is the \textit{inertia weight},
$x_{pb}$ is the position associated to the particle's personal best value, 
and $x_{gb}$ is the position associated to the global best value among all particles.

\begin{align}
v_i &= w\cdot v_i + c_1 \cdot rand() \left(x_{pb}-x_i\right)+c_2\cdot rand() \left(x_{gb}-x_i\right) \label{eq:pso_velocity} \\
x_i &= x_i + v_i \label{eq:pso_position}
\end{align}

Particles' velocity are constrained by the maximum velocity parameter $V_{max}$.
This is a very important parameter, as it determines the fineness with which regions
between the present position and the best positions known are searched.
If $V_{max}$ is too high, particles my overshoot good solutions. On the other hand,
if $V_{max}$ is too low, particles may move very slowly and may not be able to find
a good solution before the algorithm comes to a halt.

\subsubsection{Inertia Weight}
\label{sec:5.inertia}

The use of a \textit{inertia weight} $w$ has provided improved performance
in a number of applications, as an opportune selection of the inertia weight 
provides a balance between global and local exploration,
and results in fewer iterations on average to find a sufficiently good solution.

The \textit{inertia weight} w changes dynamically with the current iteration
of the algorithm according to the following law:

\begin{align*}
	w(k) = a-(a-b)\cdot \frac{k}{G},\quad k \in \left[1,G\right]
\end{align*}

\subsubsection{Acceleration Constants}
\label{sec:5.pso.constants}

The \textit{acceleration constants} $c_1$ and $c_2$ represent
the importance of the personal and global best according to the particles.
If $c_1$ is greater than $c_2$, the particles will tend to judge as better
their own personal best and will be inclined to move towards that location.


\section{Algorithm Comparison}
\label{sec:5.res}

Simulations have been run on all the algorithms proposed in the previous sections.
Each one of them has been tested with both the standard and the generalized Bouc-Wen
model of hysteresis.

Since the generalized version of the hysteresis component consists of 8 variables
it is safe to assume beforehand that the execution time of the algorithms would be higher.
A comprehensive comparison of the execution times for the algorithms is described in
Section~\ref{sec:5.comparison_time}.



\subsection{System Specifications}



\subsection{Solution Comparison}
\label{sec:5.comparison_res}

This Section serves the purpose of comparing the solutions found using the
algorithm previously described when trying to find suitable parameters
both for the classic and the generalized Bouc-Wen model of hysteresis.

%TODO: inserire dei plot per l'andamento delle variabili per tutti e 3 gli algoritmi e le 2 versioni del modello isteretico
\subsubsection{Genetic Algorithm -- Classic Bouc-Wen Model}

Table~\ref{tab:ga_classic_params} contains the parameters used
for the genetic algorithm approach. Those parameters have been used for
the genetic algorithm using all three kinds of crossover strategies:
random, weighted and proportional.

Figures~\ref{fig:ga_classic_rand},~\ref{fig:ga_classic_w}
and~\ref{fig:ga_classic_prop} show the trend of the classic Bouc-Wen model
parameters $A$, $\alpha$, $\beta$ and $\gamma$ depending on the
crossover strategy used.

It is important to note that due to how the three strategies have been developed,
the performance of the random and weighted methods depend strictly on
the initial population, as the individuals' chromosomes are always only swapped.
The final result will approach the best possible solution attainable
using the chromosomes of the initial population. 
However, it is certainly possible that the final fit may not be acceptable.

Due to this fact, the proportional crossover strategy proved to be the best
as it generates new chromosomes (and thus, completely new individuals)
each generation.

\begin{table}
	\centering
	\begin{tabular}{c c}
		\toprule
		\textbf{Parameter} & \textbf{Description} \\ \toprule
		$N$			& $30$ \\
		$G$			& $30$ \\
		$d$			& $4$  \\
		$L$			& $\left[0.5, 0.5, -0.5, -0.1\right]$ \\
		$U$			& $\left[1, 0.99, 0.5, 0.1\right]$ \\ 
		$P_{cross}$	& $0.5$ \\
		$P{mut}$	& $0.1$ \\
		$m_m$		& $0.05$ \\	\bottomrule
	\end{tabular}
	\caption{Parameters for the genetic algorithm}
	\label{tab:ga_classic_params}
\end{table}

%TODO: Per il GA sarebbe interessante mostrare le differenze tra i 3 tipi di crossover
% I dati che seguono sono stati ottenuti usando proportional

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	%	xlabel={Generation},
	ylabel={A},
	xmin=1, xmax=30,
	ymin=0.6, ymax=1,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.6,0.7,0.8,0.9,1},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,0.545135339844412)(2,0.788923360170356)(3,0.690777924711849)
		(4,0.690777924711849)(5,0.960441630068157)(6,0.690777924711849)
		(7,0.690777924711849)(8,0.615202221951477)(9,0.615202221951477)
		(10,0.690777924711849)(11,0.645962333049051)(12,0.725316820947442)
		(13,0.690777924711849)(14,0.656239028476257)(15,0.690777924711849)
		(16,0.725316820947442)(17,0.690777924711849)(18,0.656239028476257)
		(19,0.656239028476257)(20,0.725316820947442)(21,0.799661795094554)
		(22,0.799661795094554)(23,0.799661795094554)(24,0.761582661994814)
		(25,0.761582661994814)(26,0.761582661994814)(27,0.725316820947442)
		(28,0.761582661994814)(29,0.799661795094554)(30,0.761582661994814)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	%	xlabel={Generation},
	ylabel={$\alpha$},
	xmin=1, xmax=30,
	ymin=0.5, ymax=0.9,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.5,0.6,0.7,0.8,0.9},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,0.508448976427112)(2,0.754708795645205)(3,0.754708795645205)
		(4,0.657392330450923)(5,0.754708795645205)(6,0.754708795645205)
		(7,0.716973355862945)(8,0.754708795645205)(9,0.754708795645205)
		(10,0.754708795645205)(11,0.754708795645205)(12,0.754708795645205)
		(13,0.804793959442481)(14,0.754708795645205)(15,0.726326548396839)
		(16,0.804793959442481)(17,0.845033657414605)(18,0.802781974543875)
		(19,0.802781974543875)(20,0.802781974543875)(21,0.802781974543875)
		(22,0.802781974543875)(23,0.804793959442481)(24,0.804793959442481)
		(25,0.804793959442481)(26,0.804793959442481)(27,0.764554261470357)
		(28,0.764554261470357)(29,0.764554261470357)(30,0.764554261470357)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	%	xlabel={Generation},
	ylabel={$\beta$},
	xmin=1, xmax=30,
	ymin=-0.3, ymax=0.5,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.3,-0.1,0.1,0.3,0.5},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,-0.268822537450358)(2,0.344310428956600)(3,0.344310428956600)
		(4,0.344310428956600)(5,0.344310428956600)(6,0.361525950404430)
		(7,0.344310428956600)(8,0.379602247924651)(9,0.361525950404430)
		(10,0.344310428956600)(11,0.361525950404430)(12,0.344310428956600)
		(13,0.344310428956600)(14,0.344310428956600)(15,0.344310428956600)
		(16,0.344310428956600)(17,0.344310428956600)(18,0.344310428956600)
		(19,0.344310428956600)(20,0.361525950404430)(21,0.344310428956600)
		(22,0.344310428956600)(23,0.361525950404430)(24,0.344310428956600)
		(25,0.185855452192517)(26,0.185390813562035)(27,0.176562679582891)
		(28,0.185390813562035)(29,0.185390813562035)(30,0.185390813562035)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel={Generation},
	ylabel={$\gamma$},
	yticklabel style={/pgf/number format/fixed},
	xmin=1, xmax=30,
	ymin=-0.1, ymax=0.1,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.1,-0.05,0,0.05,0.1},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,-0.0871709478837318)(2,0.0852051419429015)(3,0.0852051419429015)
		(4,0.0852051419429015)(5,0.0852051419429015)(6,0.0852051419429015)
		(7,0.0852051419429015)(8,0.0852051419429015)(9,0.0852051419429015)
		(10,0.0852051419429015)(11,0.0852051419429015)(12,0.0852051419429015)
		(13,0.0894653990400466)(14,0.0894653990400466)(15,0.0852051419429015)
		(16,0.0894653990400466)(17,0.0939386689920489)(18,0.0894653990400466)
		(19,0.0939386689920489)(20,0.0939386689920489)(21,0.0939386689920489)
		(22,0.0939386689920489)(23,0.0939386689920489)(24,0.0939386689920489)
		(25,0.0464759550397010)(26,0.0487997527916861)(27,0.0464759550397010)
		(28,0.0464759550397010)(29,0.0464759550397010)(30,0.0464759550397010)
	};
	\end{axis}
	\end{tikzpicture}
	
	\caption{Genetic Algorithm -- Classic BW Model (Random crossover)}
	\label{fig:ga_classic_rand}
\end{figure}

A series of sharp changes is noticeable in Figure~\ref{fig:ga_classic_rand}.
As said beforehand, this behaviour is due to the fact that the set of chromosomes
obtainable by the individual in the population is fixed to the initial one.

Nonetheless it is possible to notice that the variables $\beta$ and $\gamma$
are the first to settle to fairly consistent values, up until generation 25.
This behaviour is probably due to the current best individual being near a local optimum.

The final values for each parameter using this crossover strategy
are in Table~\ref{tab:ga_classic_random_final}.

\begin{table}[H]
	\centering
	\begin{tabular}{l c c c c}
		\toprule
		\textbf{Parameter}		& $A$	& $\alpha$	& $\beta$	& $\gamma$ 	\\ \midrule
		\textbf{Final value}	& $0.76158$	& $0.76455$	& $,0.18539$ & $0.04647$	\\ \bottomrule
	\end{tabular}
	\caption{Random crossover -- Final values (Classic BW model)}
	\label{tab:ga_classic_random_final}
\end{table}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	%	xlabel={Generation},
	ylabel={A},
	xmin=1, xmax=30,
	ymin=0.5, ymax=0.9,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.5,0.6,0.7,0.8,0.9},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,0.618086069866413)(2,0.773856623996490)(3,0.592746940707517)
		(4,0.714602293601890)(5,0.788559995124738)(6,0.788559995124738)
		(7,0.788559995124738)(8,0.752212940633569)(9,0.789823587665247)
		(10,0.789823587665247)(11,0.789823587665247)(12,0.829314767048510)
		(13,0.829314767048510)(14,0.829314767048510)(15,0.789823587665247)
		(16,0.750332408281985)(17,0.712815787867886)(18,0.712815787867886)
		(19,0.829314767048510)(20,0.829314767048510)(21,0.789823587665247)
		(22,0.789823587665247)(23,0.829314767048510)(24,0.829314767048510)
		(25,0.829314767048510)(26,0.789823587665247)(27,0.829314767048510)
		(28,0.789823587665247)(29,0.789823587665247)(30,0.789823587665247)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	%	xlabel={Generation},
	ylabel={$\alpha$},
	xmin=1, xmax=30,
	ymin=0.5, ymax=1,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.5,0.625,0.75,0.875,1},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,0.813187272905702)(2,0.556820971784476)(3,0.860684845546224)
		(4,0.836803572892517)(5,0.893766789568932)(6,0.893766789568932)
		(7,0.806624527585961)(8,0.893766789568932)(9,0.891532372595010)
		(10,0.849078450090485)(11,0.849078450090485)(12,0.893766789568932)
		(13,0.893766789568932)(14,0.893766789568932)(15,0.893766789568932)
		(16,0.893766789568932)(17,0.936108991224760)(18,0.936108991224760)
		(19,0.936108991224760)(20,0.936108991224760)(21,0.982914440785998)
		(22,0.982914440785998)(23,0.985377885499748)(24,0.940500000000000)
		(25,0.985377885499748)(26,0.940500000000000)(27,0.936108991224760)
		(28,0.940500000000000)(29,0.940500000000000)(30,0.940500000000000)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	%	xlabel={Generation},
	ylabel={$\beta$},
	xmin=1, xmax=30,
	ymin=-0.5, ymax=0.5,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.5,-0.25,0,0.25,0.5},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,-0.499357514523178)(2,0.330664545640134)(3,-0.230611577461169)
		(4,-0.230611577461169)(5,0.487411160746289)(6,0.487411160746289)
		(7,0.460700703070708)(8,0.484948108495482)(9,-0.242142156334227)
		(10,-0.230611577461169)(11,-0.230611577461169)(12,-0.230611577461169)
		(13,-0.230611577461169)(14,-0.242142156334227)(15,-0.230611577461169)
		(16,-0.230611577461169)(17,-0.242142156334227)(18,-0.219080998588110)
		(19,-0.242142156334227)(20,-0.230611577461169)(21,-0.242142156334227)
		(22,-0.242142156334227)(23,-0.254249264150938)(24,-0.254249264150938)
		(25,-0.230611577461169)(26,-0.254249264150938)(27,-0.242142156334227)
		(28,-0.254249264150938)(29,-0.242142156334227)(30,-0.242142156334227)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	xlabel={Generation},
	ylabel={$\gamma$},
	yticklabel style={/pgf/number format/fixed},
	xmin=1, xmax=30,
	ymin=-0.1, ymax=0.1,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.1,-0.05,0,0.05,0.1},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,-0.0985378305342493)(2,0.0847351667259024)(3,0.0813828498620047)
		(4,-0.0780844050799008)(5,-0.0780844050799008)(6,-0.0780844050799008)
		(7,0.0138301733936949)(8,0.0138301733936949)(9,0.0145216820633796)
		(10,0.0145216820633796)(11,0.0145216820633796)(12,0.0152477661665486)
		(13,0.0152477661665486)(14,0.0145216820633796)(15,0.0145216820633796)
		(16,0.0145216820633796)(17,0.0145216820633796)(18,0.0145216820633796)
		(19,0.0137955979602107)(20,0.0144853778582212)(21,0.0137955979602107)
		(22,0.0152477661665486)(23,0.0152477661665486)(24,0.0160101544748761)
		(25,0.0160101544748761)(26,0.0152477661665486)(27,0.0152477661665486)
		(28,0.0152477661665486)(29,0.0144853778582212)(30,0.0160101544748761)
	};
	\end{axis}
	\end{tikzpicture}
	
	\caption{Genetic Algorithm -- Classic BW Model (Weighted crossover)}
	\label{fig:ga_classic_w}
\end{figure}

As for the random crossover strategy, the weighted crossover strategy
operates on the closed set of initial chromosome values, but combines
them judging the fit value of the individuals instead of doing it randomly.

The most important difference between these two results is the fact that
it takes fewer generations to reach what may be a global optimum.
This is especially noticeable in Figure~\ref{fig:ga_classic_w},
as the parameters $\beta$ and $\gamma$ stabilize on similar values
around generation 10 and 7, respectively.

The final values for each parameter using this crossover strategy
are in Table~\ref{tab:ga_classic_weighted_final}.

\begin{table}[H]
	\centering
	\begin{tabular}{l c c c c}
		\toprule
		\textbf{Parameter}		& $A$	& $\alpha$	& $\beta$	& $\gamma$ 	\\ \midrule
		\textbf{Final value}	& $0.78982$	& $0.94050$	& $-0.24214$ & $0.01601$	\\ \bottomrule
	\end{tabular}
	\caption{Weighted crossover -- Final values (Classic BW model)}
	\label{tab:ga_classic_weighted_final}
\end{table}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
	\begin{axis}[
%	xlabel={Generation},
	ylabel={A},
	xmin=1, xmax=30,
	ymin=0.6, ymax=1,
    grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.6,0.7,0.8,0.9,1},
	thick
	]
	
	\addplot[
	color=Maroon
	]
	coordinates {
		(1,0.846617612122516)(2,0.945169707132505)(3,0.686185321911442)
		(4,0.635391880114644)(5,0.741875275061889)(6,0.741875275061889)
		(7,0.732155512378616)(8,0.775578685813760)(9,0.743785928735175)
		(10,0.743785928735175)(11,0.757533941052583)(12,0.785476285913808)
		(13,0.785895602859595)(14,0.803612148048867)(15,0.820560794469268)
		(16,0.814418644574798)(17,0.817525500030851)(18,0.809111673028719)
		(19,0.816070097329638)(20,0.815637998888346)(21,0.813705102461688)
		(22,0.815690431932833)(23,0.815517137553180)(24,0.815517137553180)
		(25,0.815353069534198)(26,0.815868881606655)(27,0.825692496498711)
		(28,0.815308753557013)(29,0.816773883289793)(30,0.816995005947158)
	};
	
	\end{axis}
	\end{tikzpicture}

	\begin{tikzpicture}
	\begin{axis}[
	ylabel={$\alpha$},
	xmin=1, xmax=30,
	ymin=0.8, ymax=1,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.8,0.85,0.9,0.95,1},
	thick
	]
	
	\addplot[
	color=Maroon
	]
	coordinates {
		(1,0.807304104758228)(2,0.807304104758228)(3,0.944409289353833)
		(4,0.859921322048516)(5,0.916860139599848)(6,0.916860139599848)
		(7,0.888463783458492)(8,0.903226651793137)(9,0.935606267534623)
		(10,0.958038312299642)(11,0.958038312299642)(12,0.958038312299642)
		(13,0.932466246978050)(14,0.923030423935558)(15,0.911376532519616)
		(16,0.911376532519616)(17,0.894144784683887)(18,0.884143786894717)
		(19,0.894144784683887)(20,0.849437545449693)(21,0.892107817478434)
		(22,0.923568595351979)(23,0.906003036557694)(24,0.906003036557694)
		(25,0.931929326970839)(26,0.913402075446632)(27,0.933810236925943)		(28,0.933810236925943)(29,0.940500000000000)(30,0.940500000000000)	
	};
	
	\end{axis}
	\end{tikzpicture}

	\begin{tikzpicture}
	\begin{axis}[
	ylabel={$\beta$},
	xmin=1, xmax=30,
	ymin=-0.5, ymax=0.5,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.5,-0.25,0,0.25,0.5},
	thick
	]
	
	\addplot[
	color=Maroon
	]
	coordinates {
	(1,-0.383570164339162)(2,-0.209067239339800)(3,-0.346945447394620)
	(4,0.203314842137601)(5,0.211317604599334)(6,0.221883484829301)
	(7,0.134575000183786)(8,0.150040172965104)(9,0.128552630840732)
	(10,0.128552630840732)(11,0.0924247538988270)(12,0.0924247538988270)
	(13,0.0696849041430703)(14,0.0876972336644854)(15,0.0588128772541681)
	(16,0.0588128772541681)(17,0.0481057225684876)(18,0.0596059416498790)
	(19,0.0596059416498790)(20,0.0595554726646533)(21,0.0620742434800105)
	(22,0.0619325894560293)(23,0.0597591278277150)(24,0.0597591278277150)
	(25,0.0606863986765927)(26,0.0607556620243750)(27,0.0633912791535188)
	(28,0.0607681427718138)(29,0.0609861619403020)(30,0.0607868744138922)
	};
	\end{axis}
	\end{tikzpicture}

	\begin{tikzpicture}
	\begin{axis}[
	yticklabel style={/pgf/number format/fixed},
	xlabel={Generation},
	ylabel={$\gamma$},
	xmin=1, xmax=30,
	ymin=-0.1, ymax=0.1,
	grid=both,	      
	scaled y ticks = false,		
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.1,-0.05,0,0.05,0.1},
	thick
	]
	
	\addplot[
	color=Maroon
	]
	coordinates {
		(1,0.0999415052821818)(2,0.0524929987796510)(3,0.0521746568499797)
		(4,0.0296226513494849)(5,0.0547827566575863)(6,0.0296226513494849)
		(7,0.00540555464037581)(8,0.00228429919009436)(9,0.00228429919009436)
		(10,0.00228429919009436)(11,0.00498059078371382)(12,0.00234127234304952)
		(13,0.00294145297861557)(14,0.00294145297861557)(15,0.00277665812010004)
		(16,0.00261202366681348)(17,0.00271992213742177)(18,0.00293279836879291)
		(19,0.00237952420577038)(20,0.00237952420577038)(21,0.00237952420577038)
		(22,0.00269933620416253)(23,0.00245930491141351)(24,0.00273011191599481)
		(25,0.00273011191599481)(26,0.00257043305522086)(27,0.00262062997493030)
		(28,0.00261361171864668)(29,0.00261407624380850)(30,0.00273011191599481)
	};
	\end{axis}
	\end{tikzpicture}
	\caption{Genetic Algorithm -- Classic BW Model (Proportional crossover)}
	\label{fig:ga_classic_prop}
\end{figure}

In contrast to the parameter trends of random and weighted crossover strategies,
the proportional crossover strategy shows a smoother trend. 
This is especially noticeable for parameters $A$, $\beta$ and $\gamma$.

As for the weighted crossover method, it is possible to see an almost
immediate setting of parameters $\beta$ and $\gamma$, as it happens
for both around generation 7.

The final values for each parameter using this crossover strategy
are in Table~\ref{tab:ga_classic_prop_final}.

\begin{table}[H]
	\centering
	\begin{tabular}{l c c c c}
		\toprule
		\textbf{Parameter}		& $A$	& $\alpha$	& $\beta$	& $\gamma$ 	\\ \midrule
		\textbf{Final value}	& $0.81699$	& $0.94050$	& $0.06078$ & $0.00273$	\\ \bottomrule
	\end{tabular}
	\caption{Weighted crossover -- Final values (Classic BW model)}
	\label{tab:ga_classic_prop_final}
\end{table}

Figure~\ref{fig:ga_classic_res} shows how the hysteresis loop shows up
with the final parameters present in Table~\ref{tab:ga_classic_prop_final}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Images/ga_prop_final}
	\caption{Hysteresis loop result using genetic algorithm's parameters (proportional crossover approach)}
	\label{fig:ga_classic_res}
\end{figure}

\subsubsection{Modified Firefly Algorithm -- Classic Bouc-Wen Model}

Table~\ref{tab:mfa_classic_params} contains the parameters used for the
modified firefly algorithm approach. 

\begin{table}[H]
	\centering
	\begin{tabular}{c c}
		\toprule
		\textbf{Parameter} & \textbf{Description} \\ \toprule
		$N$			& $30$ \\
		$G$			& $30$ \\
		$d$			& $4$  \\
		$L$			& $\left[0, 0.5, -0.5, -0.5\right]$ \\
		$U$			& $\left[2, 0.99, 0.5, 0.5\right]$ \\ 
		$a$			& $1.1$ \\
		$b$			& $0.3$ \\ \bottomrule
	\end{tabular}
	\caption{Parameters for the genetic algorithm}
	\label{tab:mfa_classic_params}
\end{table}

Figure~\ref{fig:mfa_classic_trend} shows the trend of Bouc-Wen parameters
$A$, $\alpha$, $\beta$ and $\gamma$ using the Modified Firefly Algorithm.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
	\begin{axis}[
	ylabel={A},
	xmin=1, xmax=30,
	ymin=0.5, ymax=1.5,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.5,0.75,1,1.25,1.5},
	thick]
	
	\addplot[color=Maroon]
	coordinates {
		(1,1.06869060925092)(2,1.07031310017167)(3,0.930245427082584)
		(4,0.936679613305865)(5,0.936679613305865)(6,0.936679613305865)
		(7,0.960367546450692)(8,0.919402031004002)(9,0.919402031004002)
		(10,0.919402031004002)(11,0.927525757993710)(12,0.921703837572361)
		(13,0.921703837572361)(14,0.921703837572361)(15,0.921703837572361)
		(16,0.921703837572361)(17,0.921703837572361)(18,0.921703837572361)
		(19,0.921703837572361)(20,0.921703837572361)(21,0.928354088631171)
		(22,0.928354088631171)(23,0.928354088631171)(24,0.922151802845455)
		(25,0.922151802845455)(26,0.925594763845607)(27,0.925594763845607)
		(28,0.923707972405618)(29,0.925963112330994)(30,0.925963112330994)
	};
	
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	ylabel={$\alpha$},
	xmin=1, xmax=30,
	ymin=0.5, ymax=1,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={0.5,0.625,0.75,0.875,1},
	thick
	]
	
	\addplot[
	color=Maroon
	]
	coordinates {
		(1,0.550976413814157)(2,0.623490876234948)(3,0.777128984609912)
		(4,0.715106724722959)(5,0.715106724722959)(6,0.715106724722959)
		(7,0.830987001080587)(8,0.797556605573597)(9,0.797556605573597)
		(10,0.797556605573597)(11,0.775232834887466)(12,0.799023435255703)
		(13,0.799023435255703)(14,0.799023435255703)(15,0.799023435255703)
		(16,0.799023435255703)(17,0.799023435255703)(18,0.799023435255703)
		(19,0.799023435255703)(20,0.799023435255703)(21,0.801751730639454)
		(22,0.801751730639454)(23,0.801751730639454)(24,0.800992660349603)
		(25,0.800992660349603)(26,0.804812870702901)(27,0.804812870702901)
		(28,0.804148278131032)(29,0.806209042629755)(30,0.806209042629755)	
	};
	
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	ylabel={$\beta$},
	xmin=1, xmax=30,
	ymin=-0.5, ymax=0.5,
	grid=both,
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.5,-0.25,0,0.25,0.5},
	thick
	]
	
	\addplot[color=Maroon]
	coordinates {
		(1,0.266001570301431)(2,-0.0435908362222465)(3,0.107569352046298)
		(4,0.0310859001662921)(5,0.0310859001662921)(6,0.0310859001662921)
		(7,0.0440423469108267)(8,-0.00952233930470358)(9,-0.00952233930470358)
		(10,-0.00952233930470358)(11,0.0105856910126518)(12,0.00911806671740018)
		(13,0.00911806671740018)(14,0.00911806671740018)(15,0.00911806671740018)
		(16,0.00911806671740018)(17,0.00911806671740018)(18,0.00911806671740018)
		(19,0.00911806671740018)(20,0.00911806671740018)(21,0.00915095605160595)
		(22,0.00915095605160595)(23,0.00915095605160595)(24,0.00878879253828613)
		(25,0.00878879253828613)(26,0.00740178743614416)(27,0.00740178743614416)
		(28,0.00901025687938335)(29,0.00806867272972272)(30,0.00806867272972272)
	};
	\end{axis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{axis}[
	yticklabel style={/pgf/number format/fixed},
	xlabel={Generation},
	ylabel={$\gamma$},
	xmin=1, xmax=30,
	ymin=-0.1, ymax=0.1,
	grid=both,	      
	scaled y ticks = false,		
	grid style={line width=.1pt, draw=gray!10},
	width=12cm,height=4cm,
	xtick={0,5,10,15,20,25,30},
	ytick={-0.1,-0.05,0,0.05,0.1},
	thick
	]
	
	\addplot[color=Maroon]
	coordinates {
		(1,-0.0691733544526680)(2,-0.0207321319806590)(3,-0.00302967769376153)
		(4,-0.000441510968489555)(5,-0.000441510968489555)(6,-0.000441510968489555)
		(7,-0.00983967294117288)(8,0.000890856083589599)(9,0.000890856083589599)
		(10,0.000890856083589599)(11,-0.00369252054175541)(12,-0.000756154478913090)
		(13,-0.000756154478913090)(14,-0.000756154478913090)(15,-0.000756154478913090)
		(16,-0.000756154478913090)(17,-0.000756154478913090)(18,-0.000756154478913090)
		(19,-0.000756154478913090)(20,-0.000756154478913090)(21,-0.00155480896080897)
		(22,-0.00155480896080897)(23,-0.00155480896080897)(24,-0.00106283248509903)
		(25,-0.00106283248509903)(26,-0.00157157076493920)(27,-0.00157157076493920)
		(28,-0.00112928693805224)(29,-0.000866865925248103)(30,-0.000866865925248103)
	};
	\end{axis}
	\end{tikzpicture}
	\caption{Modified Firefly Algorithm -- Classic BW Model}
	\label{fig:mfa_classic_trend}
\end{figure}


\subsection{Time Comparison}
\label{sec:5.comparison_time}
